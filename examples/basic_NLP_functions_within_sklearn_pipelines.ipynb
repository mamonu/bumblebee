{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of basic nlp functions within ```sklearn``` pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set ups and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and our user-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from nlpfunctions.utils import *\n",
    "from nlpfunctions.basicnlp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For the purpose of this notebook, we will import the labelled text data used in 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015 (available here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "#print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_excel('Data/imdb.xlsx', header=0)\n",
    "yelp = pd.read_excel('Data/yelp_labelled.xlsx', header=0)\n",
    "\n",
    "imdb['source'] = 'imdb'\n",
    "yelp['source'] = 'yelp'\n",
    "\n",
    "df = pd.concat([imdb, yelp])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look a the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'score', 'source'], dtype='object')\n",
      "                                                text  score source\n",
      "0  A very, very, very slow-moving, aimless movie ...      0   imdb\n",
      "1  Not sure who was more lost - the flat characte...      0   imdb\n",
      "2  Attempting artiness with black & white and cle...      0   imdb\n",
      "3       Very little music or anything to speak of.        0   imdb\n",
      "4  The best scene in the movie was when Gerardo i...      1   imdb\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count                   1743\n",
      "unique                  1737\n",
      "top       Not recommended.  \n",
      "freq                       2\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>score</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>imdb</th>\n",
       "      <td>361</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yelp</th>\n",
       "      <td>499</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "score     0    1\n",
       "source          \n",
       "imdb    361  385\n",
       "yelp    499  500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df['text'].describe())   #there are some duplicates\n",
    "\n",
    "pd.crosstab(df['source'], df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df.duplicated('text')]\n",
    "df = df.drop_duplicates('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[pd.isnull(df['text'])]   #yep, 1 case\n",
    "df = df[pd.notnull(df['text'])]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include text-preprocessing functions within sklearn ```CountVectorizer()```\n",
    "\n",
    "#### Build ad-hoc pre-processor\n",
    "\n",
    "Let's remove all sentences within each text that score low on subjectivity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_preprocessor = combine_functions(sent_tokenise\n",
    "                                    , lambda x : remove_objective_sents(x, 0.3)\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build ad-hoc word-tokenisation pipeline\n",
    "\n",
    "Let's then lemmatise, mark negations, remove numeric digits and puncuation as part of our tokenisation pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tokenizer_pipe = combine_functions(word_tokenise\n",
    "                                       ,to_lower\n",
    "                                       ,POS_tagging\n",
    "                                       ,lemmatise\n",
    "                                       ,fix_neg_auxiliary\n",
    "                                       ,lambda x : remove_stopwords(x, extra_stopwords = ['x', \"'s\", 'us', 'ca',\n",
    "                                                                                      'many', 'much', 'one', 'put', 'also', 'get', 'would', 'could', 'like', 'go', 'lot', 'make'])\n",
    "                                       ,lambda s: [[re.sub(r'\\d+','',x) for x in subs] for subs in s]\n",
    "                                       ,mark_neg\n",
    "                                       ,flattenIrregularListOfLists  # now we have one list of tokens per text/paragraph\n",
    "                                       ,remove_punctuation\n",
    "                                       ,lambda x: list(filter(None, x))   # must end with a list of token lists, each sublist is a paragraph/text\n",
    "                                       )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build ad-hoc ```CountVectorizer()```\n",
    "\n",
    "We can now use our ad-hoc preprocessor and tokenizer within ```CountVectorizer```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_vec = CountVectorizer(analyzer=\"word\",\n",
    "                         preprocessor = my_preprocessor,\n",
    "                         ngram_range = (1,3),\n",
    "                         tokenizer = my_tokenizer_pipe,\n",
    "                         stop_words=None,\n",
    "                         max_features=10000,\n",
    "                         min_df=1\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build pipeline\n",
    "\n",
    "We will use the ```Transformers``` ```ColumnSelector()``` and ```Series2ListOfStrings()``` to select the ```pandas.Series``` that contains the text data and transform it into a list of strings which is ```CountVecorizer()```'s required input format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CountVectorizer().build_analyzer()(list2string(new_text.text))\n",
    "\n",
    "pipe_bags_words = Pipeline([\n",
    "        \n",
    "        ('selector', ColumnSelector(columns=['text'])),\n",
    "        \n",
    "        ('transformer', Series2ListOfStrings()),\n",
    "        \n",
    "        ('vec', my_vec)\n",
    "        \n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's apply it to our data\n",
    "\n",
    "Let's take a look a the most frequent words first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>the</td>\n",
       "      <td>and</td>\n",
       "      <td>a</td>\n",
       "      <td>I</td>\n",
       "      <td>is</td>\n",
       "      <td>of</td>\n",
       "      <td>was</td>\n",
       "      <td>to</td>\n",
       "      <td>The</td>\n",
       "      <td>this</td>\n",
       "      <td>...</td>\n",
       "      <td>one</td>\n",
       "      <td>an</td>\n",
       "      <td>great</td>\n",
       "      <td>really</td>\n",
       "      <td>all</td>\n",
       "      <td>about</td>\n",
       "      <td>by</td>\n",
       "      <td>they</td>\n",
       "      <td>from</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1012</td>\n",
       "      <td>779</td>\n",
       "      <td>618</td>\n",
       "      <td>534</td>\n",
       "      <td>473</td>\n",
       "      <td>468</td>\n",
       "      <td>461</td>\n",
       "      <td>441</td>\n",
       "      <td>354</td>\n",
       "      <td>287</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>69</td>\n",
       "      <td>68</td>\n",
       "      <td>66</td>\n",
       "      <td>62</td>\n",
       "      <td>61</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4    5    6    7    8     9   ...    40  41  \\\n",
       "word    the  and    a    I   is   of  was   to  The  this  ...   one  an   \n",
       "count  1012  779  618  534  473  468  461  441  354   287  ...    74  71   \n",
       "\n",
       "          42      43   44     45  46    47    48    49  \n",
       "word   great  really  all  about  by  they  from  time  \n",
       "count     69      68   66     62  61    59    59    59  \n",
       "\n",
       "[2 rows x 50 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n",
    "\n",
    "all_words = df['text'].str.split(expand=True).unstack().value_counts()\n",
    "all_words = all_words.to_frame().reset_index().rename(columns = {'index' : 'word', 0 : 'count'})\n",
    "\n",
    "# get 50 more frequent words, lots of \"rubbish\"\n",
    "#all_words[:50].plot.bar(x='word')\n",
    "all_words[:50].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "0    A very, very, very slow-moving, aimless movie ...\n",
      "1    Not sure who was more lost - the flat characte...\n",
      "2    Attempting artiness with black & white and cle...\n",
      "3         Very little music or anything to speak of.  \n",
      "4    The best scene in the movie was when Gerardo i...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(type(df['text']))\n",
    "print(df['text'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe_bags_words.fit_transform(df)\n",
    "\n",
    "#pd.DataFrame(pipe_bags_words.fit_transform(df).A, columns=my_vec.get_feature_names())\n",
    "\n",
    "# on new data, using the vocabulary just learned\n",
    "#pd.DataFrame(pipe_bags_words4.transform(new_text).A, columns=my_vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
